{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None  # Disable DecompressionBombError\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "D:\\anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "D:\\anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "D:\\anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:28: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image/dataset_medium/COAD_01/scale-25pc/S6.jpg\n",
      "image/dataset_medium/COAD_01/scale-25pc/S6.jpg\n"
     ]
    }
   ],
   "source": [
    "def read_csv_file():\n",
    "\n",
    "\n",
    "    DATASET_MEDIUM_DIR = 'dataset_medium.csv'\n",
    "    Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "    imgs_dirs = []\n",
    "    dataset_read_result = pd.read_csv(DATASET_MEDIUM_DIR)\n",
    "    i = 0\n",
    "    for each_img_dir, \\\n",
    "        each_landmarks_dir, \\\n",
    "        each_target_image, \\\n",
    "        each_target_landmarks, \\\n",
    "        each_status in zip(dataset_read_result['Source image'],\n",
    "                           dataset_read_result['Source landmarks'],\n",
    "                           dataset_read_result['Target image'],\n",
    "                           dataset_read_result['Target landmarks'],\n",
    "                           dataset_read_result['status']):\n",
    "        if each_status== 'training':\n",
    "            each_img_dir = 'image/dataset_medium/' + each_img_dir\n",
    "            each_landmarks_dir = 'landmark/' + each_landmarks_dir\n",
    "            each_target_image = 'image/dataset_medium/' + each_target_image\n",
    "            each_target_landmarks = 'landmark/' + each_target_landmarks\n",
    "\n",
    "            dataset_read_result.set_value(index=i, col='Source image', value=each_img_dir)\n",
    "            dataset_read_result.set_value(index=i, col='Source landmarks', value=each_landmarks_dir)\n",
    "            dataset_read_result.set_value(index=i, col='Target image', value=each_target_image)\n",
    "            dataset_read_result.set_value(index=i, col='Target landmarks', value=each_target_landmarks)\n",
    "\n",
    "            imgs_dirs.append(each_img_dir)\n",
    "            i = i + 1\n",
    "\n",
    "    print(dataset_read_result['Source image'][1])\n",
    "    print(imgs_dirs[1])\n",
    "    return dataset_read_result\n",
    "\n",
    "\n",
    "dataset_read_result = read_csv_file()\n",
    "\n",
    "# the first 10\n",
    "source_image_array = dataset_read_result['Source image']\n",
    "target_image_array = dataset_read_result['Target image']\n",
    "source_image_landmarks = dataset_read_result['Source landmarks']\n",
    "target_image_landmarks = dataset_read_result['Target landmarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class matchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        #使用super()方法调用基类的构造器，即nn.Module.__init__(self)\n",
    "        super(matchNet,self).__init__()\n",
    "        \n",
    "        \n",
    "        ## note here wantna to combined two image together\n",
    "        self.conv_0 = nn.Conv2d(3, 32, kernel_size=1)\n",
    "        self.pool_0 = torch.nn.MaxPool2d(kernel_size=1)\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(32, 64, kernel_size=1)\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=1)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(64, 128, kernel_size=1, stride=1)\n",
    "        self.conv_3 = nn.Conv2d(128, 128, kernel_size=1, stride=1)\n",
    "        self.conv_4 = nn.Conv2d(128, 64, kernel_size=1, stride=1)\n",
    "        self.pool_4 = torch.nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        \n",
    "        \n",
    "        # input based on previous , output choose from 128, 256, 512\n",
    "        self.bottleNeck = torch.nn.Linear(256, 512)\n",
    "        \n",
    "        # FC layers, input based on the previous, output choose from 128, 256, 512, 1024\n",
    "        # this layer with the softmax\n",
    "        self.fc_1 = torch.nn.Linear(1024, 256)\n",
    "        self.fc_2 = torch.nn.Linear(256, 128)\n",
    "        self.fc_3 = torch.nn.Linear(128, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        \n",
    "        \n",
    "        ########################\n",
    "        x = F.relu(self.conv_0(x))\n",
    "        x = self.pool_0(x) \n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.pool_1(x) \n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = self.pool_4(x)\n",
    "        \n",
    "        x = x.view(-1, 256)\n",
    "        x = F.relu(self.bottleNeck(x))\n",
    "        ########################\n",
    "        \n",
    "        y = F.relu(self.conv_0(y))\n",
    "        y = self.pool_0(y) \n",
    "        y = F.relu(self.conv_1(y))\n",
    "        y = self.pool_1(y) \n",
    "        y = F.relu(self.conv_2(y))\n",
    "        y = F.relu(self.conv_3(y))\n",
    "        y = F.relu(self.conv_4(y))\n",
    "        y = self.pool_4(y)\n",
    "        \n",
    "        y = y.view(-1, 256)\n",
    "        y = F.relu(self.bottleNeck(y))\n",
    "        #########################\n",
    "        \n",
    "        output =torch.cat((x, y),1)\n",
    "        \n",
    "        output = F.relu(self.fc_1(output))\n",
    "        output = F.relu(self.fc_2(output))\n",
    "        output = self.sigmoid(self.fc_3(output))\n",
    "        \n",
    "       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First using the SIFT to get the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "imgname1 = source_image_array[1]\n",
    "imgname2 = target_image_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_ratio = 50\n",
    "\n",
    "img1 = cv2.imread(imgname1)\n",
    "height1, width1, channels = img1.shape\n",
    "img2 = cv2.imread(imgname2)\n",
    "height2, width2, channels = img2.shape\n",
    "\n",
    "height1_resize = int(height1/scaling_ratio)\n",
    "width1_resize = int(width1/scaling_ratio)\n",
    "\n",
    "# print(height1) # 16308\n",
    "# print(height1_resize) #326\n",
    "\n",
    "\n",
    "height2_resize = int(height2/scaling_ratio)\n",
    "width2_resize = int(width2/scaling_ratio)\n",
    "\n",
    "# print(height2) # 13812\n",
    "# print(height2_resize) #276\n",
    "\n",
    "height_max = max(height1_resize, height2_resize)\n",
    "width_max = max(width1_resize, width2_resize )\n",
    "\n",
    "# print(height_max) # 326\n",
    "\n",
    "height1_resize_add_pad = int((height_max - height1_resize)/2)\n",
    "width1_resize_add_pad = int((width_max - width1_resize)/2)\n",
    "\n",
    "height2_resize_add_pad = int((height_max - height2_resize)/2)\n",
    "width2_resize_add_pad = int((width_max - width2_resize)/2)\n",
    "\n",
    "\n",
    "img1 = cv2.resize(img1, (width1_resize,height1_resize))\n",
    "img2 = cv2.resize(img2, (width2_resize,height2_resize))\n",
    "\n",
    "\n",
    "color = [255, 255, 255]\n",
    "image1 = cv2.copyMakeBorder(img1, height1_resize_add_pad, height1_resize_add_pad, width1_resize_add_pad, width1_resize_add_pad,cv2.BORDER_CONSTANT, value = color)\n",
    "image2 = cv2.copyMakeBorder(img2, height2_resize_add_pad, height2_resize_add_pad, width2_resize_add_pad, width2_resize_add_pad,cv2.BORDER_CONSTANT, value = color)\n",
    "image1 = cv2.resize(image1, (width_max,height_max))\n",
    "image1 = cv2.resize(image1, (width_max,height_max))\n",
    "height1, width1, channels = image1.shape\n",
    "height2, width2, channels = image2.shape\n",
    "\n",
    "\n",
    "# hmerge = np.hstack((image1, image2)) #水平拼接\n",
    "# plt.imshow(hmerge,cmap='gray')#拼接显示为gray\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, des1 = sift.detectAndCompute(image1,None)#des是描述子\n",
    "kp2, des2 = sift.detectAndCompute(image2,None)\n",
    "img3 = cv2.drawKeypoints(image1,kp1,image1,color=(255,0,255))\n",
    "img4 = cv2.drawKeypoints(image2,kp2,image2,color=(255,0,255))\n",
    "\n",
    "# hmerge = np.hstack((img3, img4)) #水平拼接\n",
    "# plt.imshow(hmerge)#拼接显示为gray\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转化 keypoint to point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.571414947509766"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp1[0].pt\n",
    "kp1[0].pt[0] # x\n",
    "kp1[0].pt[1] # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbourhood (x,y): \n",
    "    x_Upper_left = x-1\n",
    "    y_Upper_left = y-1\n",
    "    \n",
    "    x_Lower_right = x+1\n",
    "    y_Lower_right = y+1\n",
    "    \n",
    "    return x_Upper_left,y_Upper_left,x_Lower_right,y_Lower_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Upper_left,y_Upper_left,x_Lower_right,y_Lower_right = get_neighbourhood (round(kp1[0].pt[0]),round(kp1[0].pt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cropImg = image1[x_Upper_left:x_Lower_right, y_Upper_left:y_Lower_right]\n",
    "plt.imshow(cropImg)#拼接显示为gray\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_landmark_read_result = pd.read_csv(source_image_landmarks[0])\n",
    "target_landmark_read_result = pd.read_csv(target_image_landmarks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n",
      "(2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img1 = cv2.imread(imgname1)\n",
    "height1, width1, channels = img1.shape\n",
    "img2 = cv2.imread(imgname2)\n",
    "height2, width2, channels = img2.shape\n",
    "\n",
    "source_cropImg_set = []\n",
    "target_cropImg_set = []\n",
    "for source_X, \\\n",
    "    source_Y ,\\\n",
    "    target_X,\\\n",
    "    target_Y in zip(source_landmark_read_result['X'],source_landmark_read_result['Y'],target_landmark_read_result['X'],target_landmark_read_result['Y']):\n",
    "    \n",
    "    source_x_Upper_left,source_y_Upper_left,source_x_Lower_right,source_y_Lower_right = get_neighbourhood (round(source_X),round(source_Y))\n",
    "    target_x_Upper_left,target_y_Upper_left,target_x_Lower_right,target_y_Lower_right = get_neighbourhood (round(target_X),round(target_Y))\n",
    "    \n",
    "    source_cropImg = img1[source_y_Upper_left:source_y_Lower_right, source_x_Upper_left:source_x_Lower_right]\n",
    "    target_cropImg = img2[target_y_Upper_left:target_y_Lower_right, target_x_Upper_left:target_x_Lower_right]\n",
    "    \n",
    "    # height, width, channels = source_cropImg.shape\n",
    "    # print (source_x_Upper_left,source_x_Lower_right, source_y_Upper_left,source_y_Lower_right)\n",
    "    print(source_cropImg.shape)\n",
    "    source_cropImg_set.append(source_cropImg)\n",
    "    target_cropImg_set.append(target_cropImg)\n",
    "    ###### toDo use the two to make  CNN if they are the pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "source_trainloader = torch.utils.data.DataLoader(source_cropImg_set, batch_size=1,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "target_trainloader = torch.utils.data.DataLoader(target_cropImg_set, batch_size=1,\n",
    "                                          shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matchNet(\n",
       "  (conv_0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (pool_0): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (pool_1): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv_3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv_4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (pool_4): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleNeck): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc_1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc_3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = matchNet()\n",
    "labels = 1\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.DataParallel(net, list(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for source,target in zip(source_trainloader, target_trainloader):\n",
    "        # 置换 tensor from NWHC to NCWH\n",
    "        b_size = 1\n",
    "        label = torch.full((b_size,), labels,  device=device)\n",
    "        \n",
    "        \n",
    "        source=source.permute(0, 3, 1, 2)\n",
    "        target=target.permute(0, 3, 1, 2)\n",
    "        source = source.float()\n",
    "        target = target.float()\n",
    "        outputs = net(source,target)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
